{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "This notebook will derive the forward and backward (derivative) mathematical formulas of various activation functions in machine learning.\n",
    "Then we will implement the math in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Rectified Linear Unit (ReLU) activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "$$\n",
    "\\operatorname{ReLU}(x) = \\max(0, x) =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } x \\le 0, \\\\\n",
    "x, & \\text{if } x > 0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n",
      "9 9\n",
      "-7 0\n",
      "8 8\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = np.random.randint(-10, 10)\n",
    "    print(x, np.maximum(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\operatorname{ReLU}(x)}{\\partial x} =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } x < 0, \\\\\n",
    "1, & \\text{if } x > 0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6. -6.]\n",
      " [-6. -6.]] []\n",
      "[[4. 4.]\n",
      " [4. 4.]] [4. 4. 4. 4.]\n",
      "[[2. 2.]\n",
      " [2. 2.]] [2. 2. 2. 2.]\n",
      "[[9. 9.]\n",
      " [9. 9.]] [9. 9. 9. 9.]\n",
      "[[9. 9.]\n",
      " [9. 9.]] [9. 9. 9. 9.]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = np.zeros((2, 2))\n",
    "    x.fill(np.random.randint(-10, 10))\n",
    "    print(x, x[x>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "#### Forward\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99987661, 0.99987661],\n",
       "       [0.99987661, 0.99987661]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out\n",
    "\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Start with the definition of sigmoid function\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Rewrite into exponent form\n",
    "$$\n",
    "\\sigma(x) = (1 + e^{-x})^{-1}\n",
    "$$\n",
    "\n",
    "To simplify differentiation, we introduce $u$ as substitution\n",
    "$$\n",
    "\\text{Let } u = 1 + e^{-x}, \\text{ then } \\sigma(x) = u^{-1}\n",
    "$$\n",
    "\n",
    "Compute the derivative of $u$ with respect $x$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x} = -e^{-x} \\\\\n",
    "\\frac{\\partial}{\\partial u} \\left( u^{-1} \\right) = -u^{-2}\n",
    "$$\n",
    "\n",
    "Using chain rule:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial u^{-1}}{\\partial x} &= \\frac{\\partial u^{-1} }{\\partial u} \\cdot \\frac{\\partial u}{\\partial x} \\\\\n",
    "&= -u^{-2} \\cdot -e^{-x} \\\\\n",
    "&= \\frac{e^{-x}}{u^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substitute $u = 1 + e^{-x}$:\n",
    "$$\n",
    "\\frac{\\partial u^{-1}}{\\partial x} = \\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
    "$$\n",
    "\n",
    "Write the complement of $\\sigma(x)$ as:\n",
    "$$\n",
    "1 - \\sigma(x) = 1 - \\frac{1}{1 + e^{-x}} \\\\\n",
    "              = \\frac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\\n",
    "              = \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Multiplying $\\sigma(x)$ by $1 - \\sigma(x)$:\n",
    "$$\n",
    "\\sigma(x) (1 - \\sigma(x)) = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "                          = \\frac{e^{-x}}{(1 + e^{-x})^2} \\\\\n",
    "$$\n",
    "\n",
    "Therefore, we can conclude:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00012338 0.00012338]\n",
      " [0.00012338 0.00012338]]\n"
     ]
    }
   ],
   "source": [
    "grad = sigmoid(x) * (1 - sigmoid(x))\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "#### Forward\n",
    "Define the softmax function for the j-th component:\n",
    "$$\n",
    "softmax(x)_j = \\frac{e^{x_j}}{\\sum_{k=1}^{n} e^{x_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25],\n",
       "       [0.25, 0.25]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Recall the Quotient Rule.\n",
    "For a function f(x) = g(x) / h(x), the derivative is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} \\left(\\frac{g(x)}{h(x)}\\right)\n",
    "&= \\frac{g'(x) \\, h(x) - g(x) \\, h'(x)}{[h(x)]^2}. \\\\\n",
    "g(x) &= e^{x_j}, \\\\\n",
    "h(x) &= \\sum_{k=1}^{n} e^{x_k}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Compute the derivatives of g(x) and h(x) with respect to x_i.\n",
    "\n",
    "For the numerator:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x_i} \\, e^{x_j} &=\n",
    "\\begin{cases}\n",
    "e^{x_j}, & \\text{if } i = j, \\\\\n",
    "0, & \\text{if } i \\neq j.\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "For the denominator:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x_i} \\left(\\sum_{k=1}^{n} e^{x_k}\\right)\n",
    "&= e^{x_i}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Apply the quotient rule to differentiate $S_j(\\mathbf{x})$ with respect to x_i:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_j}{\\partial x_i} &= \\frac{\\frac{\\partial}{\\partial x_i}\\left(e^{x_j}\\right) \\left(\\sum_{k=1}^{n} e^{x_k}\\right) - e^{x_j}\\, \\frac{\\partial}{\\partial x_i}\\left(\\sum_{k=1}^{n} e^{x_k}\\right)}{\\left(\\sum_{k=1}^{n} e^{x_k}\\right)^2} \\\\\n",
    "&= \\frac{\\delta_{ij}\\, e^{x_j} \\left(\\sum_{k=1}^{n} e^{x_k}\\right) - e^{x_j}\\, e^{x_i}}{\\left(\\sum_{k=1}^{n} e^{x_k}\\right)^2}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Express the derivative in terms of the softmax function.\n",
    "\n",
    "Recall that:\n",
    "$$\n",
    "\\begin{align}\n",
    "S_j &= \\frac{e^{x_j}}{\\sum_{k=1}^{n} e^{x_k}}, \\\\\n",
    "S_i &= \\frac{e^{x_i}}{\\sum_{k=1}^{n} e^{x_k}}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can write:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_j}{\\partial x_i} = S_j\\left(\\delta_{ij} - S_i\\right)\n",
    "=\n",
    "\\begin{cases}\n",
    "S_j (1 - S_j), & \\text{if } i = j, \\\\\n",
    "- S_j S_i, & \\text{if } i \\neq j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally\n",
    "$$\n",
    "\\text{sum\\_term} = \\sum_{j} \\frac{\\partial L}{\\partial S_j} S_j \\\\\n",
    "\\text{probs} = S_i \\\\\n",
    "S_i \\left(\\frac{\\partial L}{\\partial S_i} - \\sum_{j} \\frac{\\partial L}{\\partial S_j} S_j\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]] [[0.125 0.125]\n",
      " [0.125 0.125]]\n"
     ]
    }
   ],
   "source": [
    "probs = softmax(x)\n",
    "grad = 1\n",
    "sum_term = np.sum(grad * probs, axis=-1, keepdims=True)\n",
    "dLdx = probs * (grad - sum_term)\n",
    "print(sum_term, dLdx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "#### Forward\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2 \\sigma (2x) - 1 \\\\\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999997, 0.99999997],\n",
       "       [0.99999997, 0.99999997]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Let\n",
    "$$\n",
    "u(x) = e^x - e^{-x} \\quad \\text{and} \\quad v(x) = e^x + e^{-x}.\n",
    "$$\n",
    "\n",
    "Then, by the quotient rule, the derivative of $\\tanh(x)$ is given by:\n",
    "$$\n",
    "\\frac{d}{dx}\\left(\\frac{u(x)}{v(x)}\\right) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}\n",
    "$$\n",
    "\n",
    "Step 1: Compute $u'(x)$ and $v'(x)$\n",
    "Differentiate $u(x)$:\n",
    "$$\n",
    "u'(x) = \\frac{d}{dx}\\left(e^x - e^{-x}\\right) = e^x + e^{-x}.\n",
    "$$\n",
    "\n",
    "Differentiate $v(x)$:\n",
    "$$\n",
    "v'(x) = \\frac{d}{dx}\\left(e^x + e^{-x}\\right) = e^x - e^{-x}.\n",
    "$$\n",
    "\n",
    "Step 2: Substitute into the quotient rule. Plug the derivatives into the quotient rule:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{\\left(e^x + e^{-x}\\right)^2}.\n",
    "$$\n",
    "\n",
    "\n",
    "Step 3: Simplify the numerator. Notice that:\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 = e^{2x} + 2 + e^{-2x},\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\left(e^x - e^{-x}\\right)^2 = e^{2x} - 2 + e^{-2x}.\n",
    "$$\n",
    "\n",
    "Subtracting these, we get:\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 - \\left(e^x - e^{-x}\\right)^2 = \\left(e^{2x} + 2 + e^{-2x}\\right) - \\left(e^{2x} - 2 + e^{-2x}\\right) = 4.\n",
    "$$\n",
    "\n",
    "\n",
    "Step 4: Write the derivative in simplified form. Thus, the derivative becomes:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{4}{\\left(e^x + e^{-x}\\right)^2}.\n",
    "$$\n",
    "\n",
    "Recall that the hyperbolic cosine is defined as:\n",
    "$$\n",
    "\\cosh(x) = \\frac{e^x + e^{-x}}{2},\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 = 4\\cosh^2(x).\n",
    "$$\n",
    "\n",
    "Substitute this into the expression for $\\tanh'(x)$:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{4}{4\\cosh^2(x)} = \\frac{1}{\\cosh^2(x)}.\n",
    "$$\n",
    "\n",
    "Since the hyperbolic secant is defined as:\n",
    "$$\n",
    "\\operatorname{sech}(x) = \\frac{1}{\\cosh(x)},\n",
    "$$\n",
    "\n",
    "we can finally write:\n",
    "$$\n",
    "\\tanh'(x) = \\operatorname{sech}^2(x).\n",
    "$$\n",
    "\n",
    "Alternatively, using the identity $\\operatorname{sech}^2(x) = 1 - \\tanh^2(x)$\n",
    "\n",
    "we also have:\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^2(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "grad = 1 - tanh(x**2)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
