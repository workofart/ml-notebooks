{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "This notebook will derive the forward and backward (derivative) mathematical formulas of various loss functions in machine learning.\n",
    "Then we will implement the math in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy\n",
    "Also known as: \"Log Loss\" or \"negative log-likelihood\"\n",
    "$$\n",
    "- \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log{\\hat{y_i}} + (1 - y_i) \\log{(1 - \\hat{y_i})} \\right]\n",
    "$$\n",
    "\n",
    "Where \n",
    "- $N$ is the total number of samples\n",
    "- $y_i$ is the true label (0 or 1) for the $i$-th sample\n",
    "- $\\hat{y_i}$ is the predicted probability for the $i$-th sample\n",
    "\n",
    "The formulation of this loss function actually comes from how we model the probability mass function (PMF) of a Bernoulli-distributed random variable $Y$, which represents a binary outcome of 0 or 1.\n",
    "The likelihood of $Y$ taking on a specific value of $y$ (either 0 or 1) is:\n",
    "\n",
    "$$\n",
    "P(Y = y) = p^y (1 - p)^{1-y}\n",
    "$$\n",
    "\n",
    "Then if we take the log-likelihood of $Y$:\n",
    "$$\n",
    "log P(Y = y) = y \\log (p) (1-y) \\log (1 - p)\n",
    "$$\n",
    "\n",
    "In our machine learning world, we want to maximize this likelihood for our model to correctly model this distribution. But since gradient descent usually is concerned with minimizing the loss function, we will convert this problem from: **maximize log likelihood** to **minimize negative log likelihood**, so we take the negation of that, which gives us exactly the first formula known as the \"Binary Cross Entropy\" loss:\n",
    "$$\n",
    "- \\left[ y \\log{\\hat{y}} + (1 - y) \\log{(1 - \\hat{y})} \\right]\n",
    "$$\n",
    "\n",
    "Then we can divide it by the number of samples $N$ in a iteration. I will omit that here since it's exactly the same.\n",
    "\n",
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] [0.99       0.59220362 0.59220362 0.59220362]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 0.52390475, 0.52390475, 0.52390475])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.ones((4,))\n",
    "y_pred = np.zeros((4,))\n",
    "y_pred.fill(np.random.rand())\n",
    "y_pred[0] = 0.99 # make the first prediction close to true label of 1\n",
    "print(y_true, y_pred)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "\n",
    "binary_cross_entropy(y_true, y_pred) # the first sample's loss should be close to 0, while the others are higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Recall\n",
    "$$\n",
    "\\text{BCE} = - \\left[ y \\log{\\hat{y}} + (1 - y) \\log{(1 - \\hat{y})} \\right] \\\\\n",
    "$$\n",
    "\n",
    "Compute the derivative separate for the two terms:\n",
    "$$\n",
    "\\frac{\\partial y \\log \\hat{y}}{\\partial \\hat{y}} = y \\cdot \\frac{1}{\\hat{y}} \\\\\n",
    "\\frac{\\partial (1 - y) \\log{(1 - \\hat{y})}}{\\partial \\hat{y}} = (1 - y) \\cdot \\frac{-1}{1-\\hat{y}} \\\\\n",
    "$$\n",
    "\n",
    "Combine terms\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = - \\left( y \\cdot \\frac{1}{\\hat{y}} + (1 - y) \\cdot \\frac{-1}{1-\\hat{y}} \\right) \\\\\n",
    "\n",
    "= - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1-\\hat{y}} \\\\\n",
    "\n",
    "= \\frac{(1 - y)(\\hat{y}) - y(1 - \\hat{y})}{\\hat{y} (1 - \\hat{y})} \\\\\n",
    "\n",
    "= \\frac{\\hat{y} - y}{\\hat{y} (1 - \\hat{y})}\n",
    "$$\n",
    "\n",
    "Now we can see that:\n",
    "\n",
    "- when $y = 1$, the gradient simplifies to $-\\frac{1}{\\hat{y}}$, which is very negative when $\\hat{y}$ is small, and pushing $\\hat{y}$ upwards closer to $y = 1$\n",
    "- when $y = 0$, the gradient simplifies to $\\frac{1}{1 - \\hat{y}}$, which is very positive when $\\hat{y}$ is large, and pushing $\\hat{y}$ downwards closer to $y = 0$\n",
    "\n",
    "The pushing of $\\hat{y}$ can be seen with this update function\n",
    "$$\n",
    "\\hat{y_{new}} = \\hat{y_{old}} - \\text{learning rate} \\cdot \\frac{\\partial \\text{BCE}}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "- So you can see, when the gradient is very positive, we push $\\hat{y}$ downwards closer to $y = 0$.\n",
    "- when the gradient is very negative, we push $\\hat{y}$ upwards closer to $y = 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] [0.99       0.59220362 0.59220362 0.59220362]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.01010101, -1.68860838, -1.68860838, -1.68860838])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bce_backward(y_true, y_pred):\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "print(y_true, y_pred)\n",
    "bce_backward(y_true, y_pred) # We should see that the gradient of the elements except the first element will be very negative, so it will push y_pred closer to y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
