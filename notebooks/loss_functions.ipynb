{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "This notebook will derive the forward and backward (derivative) mathematical formulas of various loss functions in machine learning.\n",
    "Then we will implement the math in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy\n",
    "Also known as: \"Log Loss\" or \"negative log-likelihood\"\n",
    "$$\n",
    "- \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log{\\hat{y_i}} + (1 - y_i) \\log{(1 - \\hat{y_i})} \\right]\n",
    "$$\n",
    "\n",
    "Where \n",
    "- $N$ is the total number of samples\n",
    "- $y_i$ is the true label (0 or 1) for the $i$-th sample\n",
    "- $\\hat{y_i}$ is the predicted probability for the $i$-th sample\n",
    "\n",
    "The formulation of this loss function actually comes from how we model the probability mass function (PMF) of a Bernoulli-distributed random variable $Y$, which represents a binary outcome of 0 or 1.\n",
    "The likelihood of $Y$ taking on a specific value of $y$ (either 0 or 1) is:\n",
    "\n",
    "$$\n",
    "P(Y = y) = p^y (1 - p)^{1-y}\n",
    "$$\n",
    "\n",
    "Then if we take the log-likelihood of $Y$:\n",
    "$$\n",
    "log P(Y = y) = y \\log (p) (1-y) \\log (1 - p)\n",
    "$$\n",
    "\n",
    "In our machine learning world, we want to maximize this likelihood for our model to correctly model this distribution. But since gradient descent usually is concerned with minimizing the loss function, we will convert this problem from: **maximize log likelihood** to **minimize negative log likelihood**, so we take the negation of that, which gives us exactly the first formula known as the \"Binary Cross Entropy\" loss:\n",
    "$$\n",
    "- \\left[ y \\log{\\hat{y}} + (1 - y) \\log{(1 - \\hat{y})} \\right]\n",
    "$$\n",
    "\n",
    "Then we can divide it by the number of samples $N$ in a iteration. I will omit that here since it's exactly the same.\n",
    "\n",
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] [0.99       0.68619332 0.68619332 0.68619332]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 0.37659589, 0.37659589, 0.37659589])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.ones((4,))\n",
    "y_pred = np.zeros((4,))\n",
    "y_pred.fill(np.random.rand())\n",
    "y_pred[0] = 0.99 # make the first prediction close to true label of 1\n",
    "print(y_true, y_pred)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "\n",
    "binary_cross_entropy(y_true, y_pred) # the first sample's loss should be close to 0, while the others are higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Recall\n",
    "$$\n",
    "\\text{BCE} = - \\left[ y \\log{\\hat{y}} + (1 - y) \\log{(1 - \\hat{y})} \\right] \\\\\n",
    "$$\n",
    "\n",
    "Compute the derivative separate for the two terms:\n",
    "$$\n",
    "\\frac{\\partial y \\log \\hat{y}}{\\partial \\hat{y}} = y \\cdot \\frac{1}{\\hat{y}} \\\\\n",
    "\\frac{\\partial (1 - y) \\log{(1 - \\hat{y})}}{\\partial \\hat{y}} = (1 - y) \\cdot \\frac{-1}{1-\\hat{y}} \\\\\n",
    "$$\n",
    "\n",
    "Combine terms\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = - \\left( y \\cdot \\frac{1}{\\hat{y}} + (1 - y) \\cdot \\frac{-1}{1-\\hat{y}} \\right) \\\\\n",
    "\n",
    "= - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1-\\hat{y}} \\\\\n",
    "\n",
    "= \\frac{(1 - y)(\\hat{y}) - y(1 - \\hat{y})}{\\hat{y} (1 - \\hat{y})} \\\\\n",
    "\n",
    "= \\frac{\\hat{y} - y}{\\hat{y} (1 - \\hat{y})}\n",
    "$$\n",
    "\n",
    "Now we can see that:\n",
    "\n",
    "- when $y = 1$, the gradient simplifies to $-\\frac{1}{\\hat{y}}$, which is very negative when $\\hat{y}$ is small, and pushing $\\hat{y}$ upwards closer to $y = 1$\n",
    "- when $y = 0$, the gradient simplifies to $\\frac{1}{1 - \\hat{y}}$, which is very positive when $\\hat{y}$ is large, and pushing $\\hat{y}$ downwards closer to $y = 0$\n",
    "\n",
    "The pushing of $\\hat{y}$ can be seen with this update function\n",
    "$$\n",
    "\\hat{y_{new}} = \\hat{y_{old}} - \\text{learning rate} \\cdot \\frac{\\partial \\text{BCE}}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "- So you can see, when the gradient is very positive, we push $\\hat{y}$ downwards closer to $y = 0$.\n",
    "- when the gradient is very negative, we push $\\hat{y}$ upwards closer to $y = 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] [0.99       0.68619332 0.68619332 0.68619332]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.01010101, -1.45731527, -1.45731527, -1.45731527])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bce_backward(y_true, y_pred):\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "print(y_true, y_pred)\n",
    "bce_backward(y_true, y_pred) # We should see that the gradient of the elements except the first element will be very negative, so it will push y_pred closer to y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "Let's generalize to the cross entropy loss formula, which can be used to compute the difference between two probability distributions $p_{true}$ = $y_{true}$ and $p_{pred}$ = $y_{pred}$ = $\\hat{y}$\n",
    "\n",
    "#### Forward\n",
    "\n",
    "Let's recall the binary cross entropy formula when $C = 2$:\n",
    "\n",
    "$$\n",
    "y_1 = y \\; \\text{and} \\; y_0 = 1-y \\\\\n",
    "\\hat{y}_1 = \\hat{y} \\; \\text{and} \\; \\hat{y}_0 = 1-\\hat{y} \\\\\n",
    "\\text{Cross Entropy} = \\text{Binary Cross Entropy} = -\\Bigl[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})\\Bigr]\n",
    "$$\n",
    "\n",
    "When we have more than 2 classes, let's denote the number of classes as $C$:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{Cross Entropy (CE)}\n",
    "= -\\sum_{c} y_{\\text{true}, c} \\,\\log \\bigl(y_{\\text{pred}, c}\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of ways to compute the cross entropy loss depending on the type of inputs we pass in. We will discuss each one separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. probabilities for $y_{pred}$ and one-hot encoding for $y_{true}$\n",
    "\n",
    "Inputs:\n",
    "- $y_{true}$: A **one-hot** encoded vector representing the true class (e.g., [0, 1, 0] for class 2).\n",
    "- $y_{pred}$: A probability distribution over classes (typically obtained by applying softmax to logits).\n",
    "\n",
    "What It Does:\n",
    "- The formula  $-\\sum_{c} y_{\\text{true},c} \\log(y_{\\text{pred},c})$  is the general definition of cross entropy.\n",
    "- In a **one-hot** scenario, only the term corresponding to the correct class contributes to the loss, so it simplifies to  $-\\log(\\hat{y}_{\\text{correct}})$.\n",
    "\n",
    "Loss:\n",
    "$$\n",
    "\\text{CE} = -\\sum_{c} y_{\\text{true}, c} \\,\\log \\bigl(y_{\\text{pred}, c}\\bigr).\n",
    "$$\n",
    "\n",
    "In a one-hot scenario, only the term corresponding to the correct class contributes, so it simplifies to\n",
    "\n",
    "$$\n",
    "-\\log \\bigl(y_{\\text{pred}, t}\\bigr)\n",
    "$$\n",
    "\n",
    "where $t$ is the target class index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1398379941581172 [0.01005034 0.37659589 0.37659589 0.37659589]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    return - np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "ce = cross_entropy(y_true, y_pred)\n",
    "bce = binary_cross_entropy(y_true, y_pred)\n",
    "print(ce, bce)\n",
    "np.sum(bce) == ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. logits for $y_{pred}$ and target index for $y_{true}$\n",
    "\n",
    "Inputs:\n",
    "- $\\mathbf{z} = (z_1, \\dots, z_C)$ are the logits (raw scores) from the model.\n",
    "- $t$ (an integer) is the index of the correct class.\n",
    "\n",
    "What happens internally:\n",
    "- Numerical Stability: It subtracts the maximum logit from all logits before exponentiating. This is the log-sum-exp trick that helps prevent numerical overflow.\n",
    "- Internally computes softmax which converts logits into a probability distribution $\\hat{y}_j = \\frac{e^{z_j}}{\\sum{k} e^{z_k}}$\n",
    "\n",
    "\n",
    "\n",
    "Since the true label is given as an index, the loss is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Cross Entropy} = -\\log(y_{pred, t}) = -\\log (\\text{softmax}(\\text{logits}_{target})) = -\\log\\left(\\frac{e^{z_t}}{\\sum_j e^{z_j}}\\right)\n",
    "$$\n",
    "\n",
    "Where $t$ is the target class' index, and $j$ is the all the classes\n",
    "\n",
    "This matches the generic cross entropy definition in the one-hot case (only the target class term contributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8619948040582511 [0.1553624 0.4223188 0.4223188]\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(logits, target_index):\n",
    "    max_logit = np.max(logits)\n",
    "    stable_logits = logits - max_logit\n",
    "    \n",
    "    exp_logits = np.exp(stable_logits)\n",
    "    softmax = exp_logits / sum(exp_logits)\n",
    "    \n",
    "    loss = - np.log(softmax[target_index])\n",
    "    return loss, softmax\n",
    "\n",
    "loss, softmax = cross_entropy(np.array([1,2,2]), 1)\n",
    "\n",
    "print(loss, softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "We first consider the derivative of the cross entropy with respect to the predicted probability y_{\\text{pred}} for the target class t:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}}{\\partial y_{\\text{pred}, t}}\n",
    "= - \\frac{\\partial}{\\partial y_{\\text{pred}, t}} \\bigl[\\log(y_{\\text{pred}, t})\\bigr]\n",
    "= -\\frac{1}{y_{\\text{pred}, t}}.\n",
    "$$\n",
    "\n",
    "For any other class $j \\neq t, y_{\\text{true}, j} = 0$, so the loss does not directly depend on $y_{\\text{pred}, j}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}}{\\partial y_{\\text{pred}, j}}\n",
    "= 0 \\quad \\text{(for } j \\neq t\\text{)}.\n",
    "$$\n",
    "\n",
    "**Gradient with Respect to Logits**\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$\n",
    "\\text{CE} = -\\log\\!\\Bigl(\\frac{e^{z_t}}{\\sum_{j} e^{z_j}}\\Bigr)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}, i} = \\frac{e^{z_i}}{\\sum_{k} e^{z_k}}\n",
    "$$\n",
    "\n",
    "By applying the chain rule (and using the known derivative of softmax), we arrive at the well-known result for all classes i:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}}{\\partial z_i} = y_{\\text{pred}, i} - y_{\\text{true}, i}\n",
    "$$\n",
    "\n",
    "**In more detail:**\n",
    "\n",
    "1.\tSoftmax Derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}, i}}{\\partial z_j} = y_{\\text{pred}, i}\\,\\bigl(\\delta_{ij} - y_{\\text{pred}, j}\\bigr)\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\delta_{ij}$ is the Kronecker delta (1 if $i=j$, 0 otherwise).\n",
    "\n",
    "2.\tChain Rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}}{\\partial z_i}\n",
    "= \\sum_{j} \\frac{\\partial \\text{CE}}{\\partial y_{\\text{pred}, j}}\n",
    "\\cdot \\frac{\\partial y_{\\text{pred}, j}}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "Only the term for $j = t$ is nonzero in $\\frac{\\partial \\text{CE}}{\\partial y_{\\text{pred}, j}}$. Substituting and simplifying leads to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{CE}}{\\partial z_i} &= \\bigl(y_{\\text{pred}, i} - \\delta_{it}\\bigr)\n",
    "&= y_{\\text{pred}, i} - y_{\\text{true}, i}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "1.\tTarget Class ($i = t$):\n",
    "    $$\n",
    "    \\frac{\\partial \\text{CE}}{\\partial z_t} = y_{\\text{pred}, t} - 1\n",
    "    $$\n",
    "\n",
    "    If $y_{\\text{pred}, t} < 1$, this gradient is negative, indicating $z_t$ should increase (raising the probability of the correct class).\n",
    "\n",
    "2.\tNon-Target Classes ($i \\neq t$):\n",
    "    $$\n",
    "    \\frac{\\partial \\text{CE}}{\\partial z_i} = y_{\\text{pred}, i}\n",
    "    $$\n",
    "\n",
    "    A positive gradient means $z_i$ should decrease to lower the probability for incorrect classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1553624 -0.5776812  0.4223188] [0.1553624 0.4223188 0.4223188]\n"
     ]
    }
   ],
   "source": [
    "def grad_cross_entropy(softmax, target_index):\n",
    "    grad = softmax.copy()\n",
    "    grad[target_index] -= 1  # Subtract 1 for the target index\n",
    "    return grad\n",
    "\n",
    "grad = grad_cross_entropy(softmax, target_index=1)\n",
    "print(grad, softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
