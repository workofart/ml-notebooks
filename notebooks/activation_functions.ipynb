{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### Outline:\n",
    "\n",
    "- This notebook will derive the forward (loss function calculation) and backward (derivative of activation function with respect to the prediction $\\hat{y}$) mathematical formulas of various activation functions in machine learning\n",
    "- Python implementation of both forward and backward\n",
    "\n",
    "### Motivation for activation functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks. Recall machine learning is about learning patterns from data. Oftentimes the relationship between input features and output labels are not linear (e.g. XOR \"Exclusively-OR\" digital logic gate, image classification). Given neural networks are complex function approximators, by introducing non-linearity into the neural networks, they are able to better approximate the relationship between input features and output labels.\n",
    "\n",
    "Different activation functions have different behaviors, which can affect training speed, stability, or performance on various tasks. We will look at several popular ones that are often used in building neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Rectified Linear Unit (ReLU) is one of the most widely used activation functions in deep learning, primarily because of its simplicity and effectiveness in helping networks converge faster. It zeroes out negative inputs, keeping only positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "$$\n",
    "\\operatorname{ReLU}(x) = \\max(0, x) =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } x \\le 0, \\\\\n",
    "x, & \\text{if } x > 0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 0\n",
      "5 5\n",
      "-4 0\n",
      "2 2\n",
      "-1 0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = np.random.randint(-10, 10)\n",
    "    print(x, np.maximum(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\operatorname{ReLU}(x)}{\\partial x} =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } x < 0, \\\\\n",
    "1, & \\text{if } x > 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that the derivative is not defined when $x=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2.]\n",
      " [2. 2.]] [2. 2. 2. 2.]\n",
      "[[4. 4.]\n",
      " [4. 4.]] [4. 4. 4. 4.]\n",
      "[[0. 0.]\n",
      " [0. 0.]] []\n",
      "[[9. 9.]\n",
      " [9. 9.]] [9. 9. 9. 9.]\n",
      "[[7. 7.]\n",
      " [7. 7.]] [7. 7. 7. 7.]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = np.zeros((2, 2))\n",
    "    x.fill(np.random.randint(-10, 10))\n",
    "    print(x, x[x>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "The Sigmoid (or logistic) function outputs values in the range (0, 1) and is often used for probabilities in binary classification tasks. However, it can suffer from vanishing gradients for large $\\pm x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99908895, 0.99908895],\n",
       "       [0.99908895, 0.99908895]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out\n",
    "\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Let's share the final result first because it's super simple and can be expressed neatly in terms of itself:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "For those who are curious. We will derive this step-by-step. Starting with the definition of sigmoid function:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Rewrite using exponentials\n",
    "$$\n",
    "\\sigma(x) = (1 + e^{-x})^{-1}\n",
    "$$\n",
    "\n",
    "To simplify differentiation, we introduce $u$ as substitution\n",
    "$$\n",
    "\\text{Let } u = 1 + e^{-x}, \\text{ then } \\sigma(x) = u^{-1}\n",
    "$$\n",
    "\n",
    "Compute the derivative of $u$ with respect $x$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x} = -e^{-x} \\\\\n",
    "\\frac{\\partial}{\\partial u} \\left( u^{-1} \\right) = -u^{-2}\n",
    "$$\n",
    "\n",
    "Using chain rule:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial u^{-1}}{\\partial x} &= \\frac{\\partial u^{-1} }{\\partial u} \\cdot \\frac{\\partial u}{\\partial x} \\\\\n",
    "&= -u^{-2} \\cdot -e^{-x} \\\\\n",
    "&= \\frac{e^{-x}}{u^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substitute $u = 1 + e^{-x}$:\n",
    "$$\n",
    "\\frac{\\partial u^{-1}}{\\partial x} = \\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
    "$$\n",
    "\n",
    "Write the complement of $\\sigma(x)$ as:\n",
    "$$\n",
    "1 - \\sigma(x) = 1 - \\frac{1}{1 + e^{-x}} \\\\\n",
    "              = \\frac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\\n",
    "              = \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Multiplying $\\sigma(x)$ by $1 - \\sigma(x)$:\n",
    "$$\n",
    "\\sigma(x) (1 - \\sigma(x)) = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "                          = \\frac{e^{-x}}{(1 + e^{-x})^2} \\\\\n",
    "$$\n",
    "\n",
    "Therefore, we can conclude:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00091022 0.00091022]\n",
      " [0.00091022 0.00091022]]\n"
     ]
    }
   ],
   "source": [
    "grad = sigmoid(x) * (1 - sigmoid(x))\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "The Softmax function is used to convert a vector of real numbers (“logits”) into a probability distribution over classes. It is common in multi-class classification tasks (e.g., final layer of a neural network for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "Define the softmax function for the j-th component:\n",
    "$$\n",
    "softmax(x)_j = \\frac{e^{x_j}}{\\sum_{k=1}^{n} e^{x_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25],\n",
       "       [0.25, 0.25]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # for numeric stability\n",
    "    shifted_x = x - np.max(x)\n",
    "    return np.exp(shifted_x) / np.sum(np.exp(shifted_x))\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Recall the Quotient Rule.\n",
    "For a function f(x) = g(x) / h(x), the derivative is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} \\left(\\frac{g(x)}{h(x)}\\right)\n",
    "&= \\frac{g'(x) \\, h(x) - g(x) \\, h'(x)}{[h(x)]^2} \\\\\n",
    "g(x) &= e^{x_j} \\\\\n",
    "h(x) &= \\sum_{k=1}^{n} e^{x_k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Compute the derivatives of $g(x)$ and $h(x)$ with respect to $x_i$.\n",
    "\n",
    "For the numerator:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x_i} \\, e^{x_j} &=\n",
    "\\begin{cases}\n",
    "e^{x_j}, & \\text{if } i = j, \\\\\n",
    "0, & \\text{if } i \\neq j.\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "For the denominator:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x_i} \\left(\\sum_{k=1}^{n} e^{x_k}\\right)\n",
    "&= e^{x_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Apply the quotient rule to differentiate $S_j(\\mathbf{x})$ with respect to x_i:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_j}{\\partial x_i} &= \\frac{\\frac{\\partial}{\\partial x_i}\\left(e^{x_j}\\right) \\left(\\sum_{k=1}^{n} e^{x_k}\\right) - e^{x_j}\\, \\frac{\\partial}{\\partial x_i}\\left(\\sum_{k=1}^{n} e^{x_k}\\right)}{\\left(\\sum_{k=1}^{n} e^{x_k}\\right)^2} \\\\\n",
    "&= \\frac{\\delta_{ij}\\, e^{x_j} \\left(\\sum_{k=1}^{n} e^{x_k}\\right) - e^{x_j}\\, e^{x_i}}{\\left(\\sum_{k=1}^{n} e^{x_k}\\right)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Express the derivative in terms of the softmax function.\n",
    "\n",
    "Recall that:\n",
    "$$\n",
    "\\begin{align}\n",
    "S_j &= \\frac{e^{x_j}}{\\sum_{k=1}^{n} e^{x_k}} \\\\\n",
    "S_i &= \\frac{e^{x_i}}{\\sum_{k=1}^{n} e^{x_k}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can write:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_j}{\\partial x_i} = S_j\\left(\\delta_{ij} - S_i\\right)\n",
    "=\n",
    "\\begin{cases}\n",
    "S_j (1 - S_j), & \\text{if } i = j, \\\\\n",
    "- S_j S_i, & \\text{if } i \\neq j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally\n",
    "$$\n",
    "\\text{sum\\_term} = \\sum_{j} \\frac{\\partial L}{\\partial S_j} S_j \\\\\n",
    "\\text{probs} = S_i \\\\\n",
    "S_i \\left(\\frac{\\partial L}{\\partial S_i} - \\sum_{j} \\frac{\\partial L}{\\partial S_j} S_j\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]] [[0.125 0.125]\n",
      " [0.125 0.125]]\n"
     ]
    }
   ],
   "source": [
    "probs = softmax(x)\n",
    "grad = 1\n",
    "sum_term = np.sum(grad * probs, axis=-1, keepdims=True)\n",
    "dLdx = probs * (grad - sum_term)\n",
    "print(sum_term, dLdx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "The Hyperbolic Tangent (tanh) function is similar to sigmoid but outputs values in (-1, 1). It is often preferred over sigmoid in hidden layers because it’s zero-centered, which can help optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2 \\sigma (2x) - 1 \\\\\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999834, 0.99999834],\n",
       "       [0.99999834, 0.99999834]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "Let\n",
    "$$\n",
    "u(x) = e^x - e^{-x} \\quad \\text{and} \\quad v(x) = e^x + e^{-x}\n",
    "$$\n",
    "\n",
    "Then, by the quotient rule, the derivative of $\\tanh(x)$ is given by:\n",
    "$$\n",
    "\\frac{d}{dx}\\left(\\frac{u(x)}{v(x)}\\right) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}\n",
    "$$\n",
    "\n",
    "Step 1: Compute $u'(x)$ and $v'(x)$\n",
    "Differentiate $u(x)$:\n",
    "$$\n",
    "u'(x) = \\frac{d}{dx}\\left(e^x - e^{-x}\\right) = e^x + e^{-x}\n",
    "$$\n",
    "\n",
    "Differentiate $v(x)$:\n",
    "$$\n",
    "v'(x) = \\frac{d}{dx}\\left(e^x + e^{-x}\\right) = e^x - e^{-x}\n",
    "$$\n",
    "\n",
    "Step 2: Substitute into the quotient rule. Plug the derivatives into the quotient rule:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{\\left(e^x + e^{-x}\\right)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Step 3: Simplify the numerator. Notice that:\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 = e^{2x} + 2 + e^{-2x}\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\left(e^x - e^{-x}\\right)^2 = e^{2x} - 2 + e^{-2x}\n",
    "$$\n",
    "\n",
    "Subtracting these, we get:\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 - \\left(e^x - e^{-x}\\right)^2 = \\left(e^{2x} + 2 + e^{-2x}\\right) - \\left(e^{2x} - 2 + e^{-2x}\\right) = 4\n",
    "$$\n",
    "\n",
    "\n",
    "Step 4: Write the derivative in simplified form. Thus, the derivative becomes:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{4}{\\left(e^x + e^{-x}\\right)^2}\n",
    "$$\n",
    "\n",
    "Recall that the hyperbolic cosine is defined as:\n",
    "$$\n",
    "\\cosh(x) = \\frac{e^x + e^{-x}}{2}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\left(e^x + e^{-x}\\right)^2 = 4\\cosh^2(x)\n",
    "$$\n",
    "\n",
    "Substitute this into the expression for $\\tanh'(x)$:\n",
    "$$\n",
    "\\tanh'(x) = \\frac{4}{4\\cosh^2(x)} = \\frac{1}{\\cosh^2(x)}\n",
    "$$\n",
    "\n",
    "Since the hyperbolic secant is defined as:\n",
    "$$\n",
    "\\operatorname{sech}(x) = \\frac{1}{\\cosh(x)}\n",
    "$$\n",
    "\n",
    "we can finally write:\n",
    "$$\n",
    "\\tanh'(x) = \\operatorname{sech}^2(x)\n",
    "$$\n",
    "\n",
    "Alternatively, using the identity $\\operatorname{sech}^2(x) = 1 - \\tanh^2(x)$\n",
    "\n",
    "we also have:\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.32610934e-06 3.32610934e-06]\n",
      " [3.32610934e-06 3.32610934e-06]]\n"
     ]
    }
   ],
   "source": [
    "grad = 1 - tanh(x)**2\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
